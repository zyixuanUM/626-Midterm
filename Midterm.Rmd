---
title: "Midterm"
output: html_document
date: "2023-04-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

Import data
```{r}
training_data <- read.table("/Users/cengyixuan/Documents/626/Midterm/training_data.txt", header = T)
test_data <- read.table("/Users/cengyixuan/Documents/626/Midterm/test_data.txt", header = T)

# Construct binary and multi class variable
training_data$binary = factor(ifelse(training_data$activity > 3, 0, 1))
training_data$multi = factor(ifelse(training_data$activity > 6, 7, training_data$activity))

training_data <- training_data[,-c(1,2)]
test_data <- test_data[, -1]
```

```{r}
# Create Training Data
ind <- sample(nrow(training_data), nrow(training_data) * 0.7)
training <- training_data[ind,]
testing <- training_data[-ind,]

training1 <- training[, - 563]
training2 <- training[, - 562]
testing1 <- testing[, - 563]
testing2 <- testing[, - 562]
```

**1. Binary Classifier**
a. Logistic Regression
```{r}
glm.fits = glm(binary ~ .,data=training1, family=binomial)
glm.probs = predict(glm.fits, testing1, type="response")
glm.pred = rep(0, dim(testing1)[1])
glm.pred[glm.probs>0.5] = 1
table(glm.pred, testing1$binary)
mean(glm.pred == testing1$binary) # 1
```

b. GLM with elastic net
```{r}
library(glmnet)
library(caret)
# Set up 10-fold cross validation
trctrl <- trainControl(method = "repeatedcv", repeats = 10)

# Train elastic net model
enetFit <- train(binary ~., data = training1, 
                 method = "glmnet",
                 trControl = trctrl,
                 # alpha and lambda parameters to try
                 tuneGrid = data.frame(alpha=0.5,
                                       lambda=seq(0.1,0.9,0.1)))

# best alpha and lambda values by cross-validation accuracy
enetFit$bestTune

# test accuracy 
class.res <- predict(enetFit,testing1[,-562])
confusionMatrix(testing[,562],class.res)$overall[1] # 0.990133 
```

c. Lasso regression
```{r}
# Run cross-validation and select best lambda
mod_cv <- cv.glmnet(x = as.matrix(training1[,1:561]),y = training1$binary, 
                    family = "binomial",intercept = F, alpha = 1)
best_lambda <- mod_cv$lambda.min

# Set up Lasso regression
la.fit <- glmnet(x = as.matrix(training1[,1:561]),y = training1$binary,
                 family = "binomial", alpha = 1, lambda = best_lambda)
class.probs <- predict(la.fit,s = best_lambda, newx = as.matrix(testing1[,-562]))
class.pred=rep(0, dim(testing1)[1])
class.pred[class.probs>0.5] = 1
table(class.pred, testing1$binary)
mean(class.pred == testing1$binary) # 1
```

d. Ridge regression
```{r}
cv_model <- cv.glmnet(x = as.matrix(training1[,1:561]),y = training1$binary, 
                    family = "binomial",intercept = F, alpha = 0)
best_lambda2 <- cv_model$lambda.min

# Set up Ridge regression
ridge.fit <- glmnet(x = as.matrix(training1[,1:561]),y = training1$binary,
                 family = "binomial", alpha = 0, lambda = best_lambda2)
class.probs2 <- predict(ridge.fit,s = best_lambda2, newx = as.matrix(testing1[,-562]))
class.pred2 = rep(0, dim(testing1)[1])
class.pred2[class.probs2>0.5] = 1
mean(class.pred2 == testing1$binary) #0.999142
```

e. Linear Discriminant Analysis
```{r}
library(MASS)
lda.fit <- lda(binary ~., data = training1)
lda.pred <- predict(lda.fit, testing1)
lda.class = lda.pred$class
table(lda.class, testing1$binary)
mean(lda.class == testing1$binary) # 1
```

f. SVM 
```{r}
library(e1071)
svm.fit <- train(binary ~., data = training1, method = "svmLinear",
                  trControl = trctrl)
svm.pred <- predict(svm.fit, newdata = testing1)
table(svm.pred, testing1$binary)
mean(svm.pred == testing1$binary) # 1

svm_fit = svm(binary~., data=training1, kernel="linear", 
              cost=10, scale=FALSE)
svm.pred = predict(svm.fit, testing1)
table(svm.pred,testing1$binary)
mean(svm.pred == testing1$binary) # 1
```

SVM with radial
```{r}
svm_fit2 = svm(binary~., data=training1, kernel="radial", 
               cost=10, scale=FALSE)
svm.pred2 = predict(svm_fit2, testing1)
table(svm.pred2,testing1$binary)
mean(svm.pred2 == testing1$binary) # 0.999571
```

g. Single Hidden-layer Neural Network
```{r}
library(neuralnet)
nn.fit = neuralnet(binary ~., data=training1,
                   hidden = 3, act.fct = "logistic",
                   linear.output=F)
nn.test = compute(nn.fit, testing1)
nn.prob = nn.test$net.result[,2]
nn.pred = rep(0, dim(testing1)[1])
nn.pred[nn.prob>=0.5] = 1
table(nn.pred, testing1$binary)
mean(nn.pred == testing1$binary) # 1
```

i. Adaboost
```{r}
library(adabag)
ada.fit <- boosting(binary~., data=training1, mfinal=10)
ada.prob = predict(ada.fit, testing1)$prob[,2]
ada.pred = rep(0,length(ada.prob))
ada.pred[ada.prob>=0.5]= 1
table(ada.pred, testing1$binary)
mean(ada.pred == testing1$binary) # 0.999142
```

submit:
```{r}
data1 = training_data[, -563]
trControl <- trainControl(method = "cv", number = 10)
svm_submit1 <- train(binary ~., data = data1, method = "svmLinear",
                 trControl = trControl)
test.svm_submit1 <- predict(svm_submit1, newdata = test_data)
submit1 <- data.frame(test.svm_submit1 = as.numeric(test.svm_submit1) - 1)
write.table(submit1, file = "binary_2516.txt", row.names = F, col.names = F)
```

**2. Multi Classifier**
a. bagging
```{r}
library(adabag)
library(rpart)
bag = bagging(multi ~ ., data = training2, mfinal = 10, 
              control = rpart.control(maxdepth = 6))
bag.pred = predict(bag, testing2)
bag.pred$confusion
1 - bag.pred$error # 0.8837409
```

b. Adaboost
```{r}
boo = boosting(multi ~ ., data = training2, mfinal = 10,
               control = rpart.control(maxdepth = 6))
boo.pred = predict(boo, testing2)
boo.pred$confusion
1 - boo.pred$error # 0.967825
```

c. SVM
```{r}
svm_linear = svm(multi~., data=training2, kernel="linear", 
              cost=10, scale=FALSE)
svm_linear.pred = predict(svm_linear, testing2)
table(svm_linear.pred,testing2$multi)
mean(svm_linear.pred == testing2$multi) # 0.985414
```

```{r}
svm.radial <- train(multi ~., data = training2, method = "svmRadial",
                    trControl = ctrl)
svm.radial.pred <- predict(svm.radial, newdata = testing2)
table(svm.radial.pred, testing2$multi)
mean(svm.radial.pred == testing2$multi) #  0.974689
```

d. randomForest
```{r}
library(randomForest)
rf <- randomForest(multi ~., data = training2, importance = T)
rf.pred <- predict(rf, testing2[, -562], type = "class")
confusionMatrix(rf.pred, testing2$multi)$overall[1] # 0.980695
```

e. bp neural network
```{r}
library(neuralnet)
library(dplyr)
bpnn <- neuralnet(multi ~., data = training2,
                  hidden = 6, err.fct = "ce",
                  linear.output=F, 
                  stepmax = 100000, threshold = 0.01)

nntest <- testing2
nntest$one <- ifelse(testing2$multi == 1, 1, 0)
nntest$two <- ifelse(testing2$multi == 2, 1, 0)
nntest$thr <- ifelse(testing2$multi == 3, 1, 0)
nntest$fou <- ifelse(testing2$multi == 4, 1, 0)
nntest$fiv <- ifelse(testing2$multi == 5, 1, 0)
nntest$six <- ifelse(testing2$multi == 6, 1, 0)
nntest$sev <- ifelse(testing2$multi == 7, 1, 0)

bpnn.result <- compute(bpnn, nntest)
bpnn.pred <- c("one", "two", "thr", "fou", "fiv", "six", "sev")[apply(bpnn.result$net.result, 1, which.max)]%>%
  as.data.frame()

bpnn.pred.num <- bpnn.pred %>%
  mutate(pred = case_when(
    bpnn.pred == 'one' ~ 1,
    bpnn.pred == 'two' ~ 2,
    bpnn.pred == 'thr' ~ 3,
    bpnn.pred == 'fou' ~ 4,
    bpnn.pred == 'fiv' ~ 5,
    bpnn.pred == 'six' ~ 6,
    bpnn.pred == 'sev' ~ 7,
    ))

bpnn_pred <- bpnn.pred.num[[2]] %>% as.factor()
confusionMatrix(bpnn_pred, testing2$multi)$overall[1] #0.96139 
```

f. LDA
```{r}
library(MASS)
lda <- lda(multi ~., data = training2)
lda_pred <- predict(lda, testing2)
table(lda_pred$class, testing2$multi)
mean(lda_pred$class == testing2$multi) # 0.980695
```

submit: 
```{r}
data2 = training_data[, -562]
svm_submit2 <- train(multi ~., data = data2,method = "svmLinear", trControl = ctrl)
test.svm_submit2 <- predict(svm_submit2, newdata = test_data)
submit2 <- data.frame(test.svm_submit2 = as.numeric(test.svm_submit2))
write.table(submit2, file = "multiclass_2516.txt", row.names = F, col.names = F)
```

